{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Kavyansh Tyagi/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'midas.dpt_depth'; 'midas' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(classifier\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_classifier.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 103\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 72\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming the first image is real and the second is a spoof\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Load MiDaS model\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m midas_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_midas_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Generate depth maps for the images\u001b[39;00m\n\u001b[0;32m     75\u001b[0m depth_maps \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mload_midas_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_midas_model\u001b[39m():\n\u001b[1;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintel-isl/MiDaS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMiDaS_small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Kavyansh Tyagi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:570\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    567\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    568\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 570\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Kavyansh Tyagi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:596\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[0;32m    595\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 596\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    599\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kavyansh Tyagi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:108\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    106\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\hubconf.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdpt_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPTDepthModel\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmidas_net\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MidasNet\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmidas_net_custom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MidasNet_small\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'midas.dpt_depth'; 'midas' is not a package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the MiDaS model from torch.hub\n",
    "def load_midas_model():\n",
    "    model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Generate depth map using MiDaS\n",
    "def generate_depth_map(model, image):\n",
    "    with torch.no_grad():\n",
    "        input_image = preprocess_image(image).unsqueeze(0)\n",
    "        depth_map = model(input_image)\n",
    "        depth_map = torch.nn.functional.interpolate(depth_map.unsqueeze(1), size=(256, 256), mode='bilinear', align_corners=False)\n",
    "        depth_map = depth_map.squeeze().cpu().numpy()\n",
    "    return depth_map\n",
    "\n",
    "# Define a simple classifier to distinguish between real and spoofed faces\n",
    "def create_classifier():\n",
    "    classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(256 * 256, 512),  # Assuming depth map size of 256x256\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)  # Two classes: real (0) and spoof (1)\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "# Train the classifier\n",
    "def train_classifier(classifier, dataloader, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            images = images.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Paths to your images and corresponding labels (0 for real, 1 for spoof)\n",
    "    image_paths = [\"path_to_real_image.jpg\", \"path_to_spoof_image.jpg\"]\n",
    "    labels = [0, 1]  # Assuming the first image is real and the second is a spoof\n",
    "\n",
    "    # Load MiDaS model\n",
    "    midas_model = load_midas_model()\n",
    "\n",
    "    # Generate depth maps for the images\n",
    "    depth_maps = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        depth_map = generate_depth_map(midas_model, image)\n",
    "        depth_maps.append(depth_map)\n",
    "\n",
    "    # Convert depth maps and labels to PyTorch tensors\n",
    "    depth_maps = np.array(depth_maps)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Convert depth maps and labels to PyTorch tensors\n",
    "    depth_maps_tensor = torch.tensor(depth_maps).float()\n",
    "    labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "    # Create a dataset and dataloader\n",
    "    dataset = TensorDataset(depth_maps_tensor, labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    # Initialize the classifier\n",
    "    classifier = create_classifier()\n",
    "\n",
    "    # Train the classifier\n",
    "    train_classifier(classifier, dataloader, epochs=10)\n",
    "\n",
    "    # Save the trained classifier\n",
    "    torch.save(classifier.state_dict(), \"depth_classifier.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2420262892.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    from ..MiDaS/midas.dpt_depth import DPTDepthModel\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from midas.dpt_depth import DPTDepthModel\n",
    "from midas.transforms import Resize, NormalizeImage, PrepareForNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureFusionBlock, Interpolate, _make_encoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMidasNet\u001b[39;00m(BaseModel):\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from .base_model import BaseModel\n",
    "from .blocks import FeatureFusionBlock, Interpolate, _make_encoder\n",
    "\n",
    "\n",
    "class MidasNet(BaseModel):\n",
    "    \"\"\"Network for monocular depth estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=None, features=256, non_negative=True):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        Args:\n",
    "            path (str, optional): Path to saved model. Defaults to None.\n",
    "            features (int, optional): Number of features. Defaults to 256.\n",
    "            backbone (str, optional): Backbone network for encoder. Defaults to resnet50\n",
    "        \"\"\"\n",
    "        print(\"Loading weights: \", path)\n",
    "\n",
    "        super(MidasNet, self).__init__()\n",
    "\n",
    "        use_pretrained = False if path is None else True\n",
    "\n",
    "        self.pretrained, self.scratch = _make_encoder(backbone=\"resnext101_wsl\", features=features, use_pretrained=use_pretrained)\n",
    "\n",
    "        self.scratch.refinenet4 = FeatureFusionBlock(features)\n",
    "        self.scratch.refinenet3 = FeatureFusionBlock(features)\n",
    "        self.scratch.refinenet2 = FeatureFusionBlock(features)\n",
    "        self.scratch.refinenet1 = FeatureFusionBlock(features)\n",
    "\n",
    "        self.scratch.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),\n",
    "            Interpolate(scale_factor=2, mode=\"bilinear\"),\n",
    "            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(True) if non_negative else nn.Identity(),\n",
    "        )\n",
    "\n",
    "        if path:\n",
    "            self.load(path)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): input data (image)\n",
    "\n",
    "        Returns:\n",
    "            tensor: depth\n",
    "        \"\"\"\n",
    "\n",
    "        layer_1 = self.pretrained.layer1(x)\n",
    "        layer_2 = self.pretrained.layer2(layer_1)\n",
    "        layer_3 = self.pretrained.layer3(layer_2)\n",
    "        layer_4 = self.pretrained.layer4(layer_3)\n",
    "\n",
    "        layer_1_rn = self.scratch.layer1_rn(layer_1)\n",
    "        layer_2_rn = self.scratch.layer2_rn(layer_2)\n",
    "        layer_3_rn = self.scratch.layer3_rn(layer_3)\n",
    "        layer_4_rn = self.scratch.layer4_rn(layer_4)\n",
    "\n",
    "        path_4 = self.scratch.refinenet4(layer_4_rn)\n",
    "        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n",
    "        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n",
    "        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n",
    "\n",
    "        out = self.scratch.output_conv(path_1)\n",
    "\n",
    "        return torch.squeeze(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MiDaS model\n",
    "def load_midas_model():\n",
    "    model = DPTDepthModel(\n",
    "        path=\"weights/dpt_large-midas-2f21e586.pt\",\n",
    "        backbone=\"vitl16_384\",\n",
    "        non_negative=True,\n",
    "    )\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Generate depth map using MiDaS\n",
    "def generate_depth_map(model, image):\n",
    "    with torch.no_grad():\n",
    "        input_image = preprocess_image(image).unsqueeze(0)\n",
    "        depth_map = model(input_image)\n",
    "        depth_map = torch.nn.functional.interpolate(depth_map.unsqueeze(1), size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        depth_map = depth_map.squeeze().cpu().numpy()\n",
    "    return depth_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames and display depth map\n",
    "def capture_and_display(midas_model):\n",
    "    cap = cv2.VideoCapture(0)  # Start video capture from webcam\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Generate depth map\n",
    "        depth_map = generate_depth_map(midas_model, frame)\n",
    "\n",
    "        # Display depth map using Matplotlib\n",
    "        plt.imshow(depth_map, cmap='plasma')\n",
    "        plt.colorbar()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        # Wait for 'q' key press to capture the next frame or exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            plt.close()  # Close the Matplotlib window\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Load MiDaS model\n",
    "    midas_model = load_midas_model()\n",
    "\n",
    "    # Capture and display depth map\n",
    "    capture_and_display(midas_model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
